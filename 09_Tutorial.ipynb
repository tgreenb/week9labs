{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21277bfb-4f29-47c0-9329-b752a0c9b37a",
   "metadata": {},
   "source": [
    "UM MSBA - BGEN632\n",
    "\n",
    "# Week 9: Statistics in Python\n",
    "\n",
    "The purpose of this tutorial is to help you become familiar with performing descriptive and inferential statistics in Python. \n",
    "\n",
    "We will begin with a brief overview of statistics to introduce basic terms and concepts. You may already be familiar with them, but we will cover them to ensure we have a bit of common foundational knowledge given the variety of students in our course. \n",
    "\n",
    "We will then move on to a high-level discussion of statistical approaches and their implementation in Python. \n",
    "1. Collecting data and analyzing it (*descriptive* statistics)\n",
    "    * Important step in familiarizing yourself with data\n",
    "    * Helps determine type of analysis that can be perform on data\n",
    "2. Using this to draw conclusions about a population\n",
    "(*inferential* statistics)\n",
    "    * Includes both predictive and prescriptive analysis\n",
    "\n",
    "---\n",
    "\n",
    "Let's prepare out notebook before jumping into some math.\n",
    "\n",
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637013dd-7073-4c8e-b49f-cc26375178d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts # for QQ plot\n",
    "from scipy.stats import pearsonr # correlation p-values\n",
    "from sklearn.linear_model import LinearRegression # regression output\n",
    "import statsmodels.formula.api as smf # ols regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbd39e-80db-4b7b-a53a-b80c874b3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(\"/Users/obn/Documents/GitHub/UM-BGEN632/week9labs/data\")  # change this to your filepath\n",
    "os.getcwd()  # confirm change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d32f6-715c-47a2-b2d7-8f41ba6f83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ozone data and do quick inspect\n",
    "ozone_df = pd.read_table(\"ozone.data.txt\")\n",
    "ozone_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75aa9a-8cec-47e2-a886-9839b1b81b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load car data and do quick inspect\n",
    "car_df = pd.read_table('car.test.frame.txt', sep='\\t')\n",
    "car_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13da05-db22-4eb1-a46b-3bf870f2150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load student data and do quick inspect\n",
    "student_df = pd.read_table('class_performance.txt')\n",
    "student_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40aeac-b270-4bca-9a53-3aff762c1f7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistics Overview\n",
    "\n",
    "The real world is random and uncertain. We need a way to describe, predict, and draw conclusions from observations in the real world. We do this using **statistics**. \n",
    "\n",
    "In the statistical analysis of data, we typically use data from a few selected samples to draw conclusions about the population from which these samples were taken. \n",
    "  * *Population* - includes all of the elements from a set of data; the thing in which we are interested . . . \"truth\".\n",
    "  * *Parameters* - the defining characteristics of a population.\n",
    "\n",
    "Since the parameters characterizing the population are unknown, we have to use samples to obtain *estimates* of those parameters. When estimating a parameter of a population, we typically cannot measure all subjects or elements. In other words, we usually cannot accurately know populations or the parameters describing a population.\n",
    "\n",
    "We have to limit ourselves to investigating a *sample* taken from this group. We have to collect examples from the population and characterize these examples. Correct study design should ensure that the sample data are *representative* of the population from which the samples were taken.\n",
    "\n",
    "* *Sample* - subset of a population that we collect and can describe; consists of one or more observations from the population.\n",
    "\n",
    "Based on the *sample statistic*, i.e., the corresponding value calculated from the sample data, we use statistical inference to draw conclusions about the corresponding parameter in the population.\n",
    "\n",
    "* *Statistical inference* — enables you to make an educated guess about a population parameter based on a statistic computed from a representative sample from that population.\n",
    "\n",
    "Examples of parameters and statistics are provided below. Population parameters are often indicated using Greek letters, while sample statistics typically use standard letters.\n",
    "\n",
    "|  | Sample Statistic | Population Parameter | \n",
    "|:----: |:----: |:----: |\n",
    "| Mean               | $\\bar{x}$ | $\\mu $ |\n",
    "| Standard Deviation | $sd$ | $\\sigma$ |\n",
    "\n",
    "### Point vs. Summary Statistic\n",
    "\n",
    "Sometimes we use the word \"statistic\" to refer to a characteristic of data. \n",
    "\n",
    "* *Point* Statistic - a specific measured value (e.g., I am 1.524 meters tall).\n",
    "* *Summary* Statistic - a value representing a characteristic of many values (e.g., average height of UM faculty).\n",
    "  \n",
    "Summary statistics are a building block for analytical work. Summaries are abstract *descriptions* of a sample or population. Distributions of summary statistics can be used to make *inferences* about a population.\n",
    "\n",
    "### Exploratory Analysis to Inference\n",
    "\n",
    "Think about sampling as something you are cooking:\n",
    "* Taste (examine) a small part of what you're cooking to get an idea about the dish as a whole.\n",
    "* Once tasted, decide this spoonful isn't salty enough, that's *exploratory analysis*.\n",
    "* If generalized and conclude that the dish needs salt, that's an *inference*.\n",
    "\n",
    "To be valid, a sample must be *representative* of the population. If samples are not representative, they are *biased*.\n",
    "\n",
    "Bias can have a number of sources:\n",
    "* The selection of subjects.\n",
    "* The structure of the experiment.\n",
    "* The measurement device.\n",
    "* The analysis of the data.\n",
    "\n",
    "Selection of Subjects Example: *Sampling Bias in Surveys*\n",
    "* Non-response - only a small fraction of randomly sampled  people choose to respond.\n",
    "* Voluntary response - sample consists of people who volunteer to respond because they have strong opinions on the issue.\n",
    "* Convenience sample - easily accessible individuals are more likely to be included in the sample.\n",
    "\n",
    "Ultimately, care should be taken to avoid bias in the data as much as possible.\n",
    "\n",
    "### Obtaining Good Samples\n",
    "\n",
    "Statistical methods are based on implied randomness. Usually (reliable) sampling involves collecting data using some random framework. Randomization is used to avoid bias as much as possible, and there are different ways to randomize an experiment, including sampling. \n",
    "\n",
    "* Common random sampling techniques:\n",
    "  * Simple sampling - randomly select cases from the population, no implied connection between the selected points.\n",
    "  * Stratified sampling - strata made up of similar observations; take a simple random sample from each stratum.\n",
    "  * Cluster sampling - clusters not homogeneous observations; take a simple random sample from a random sample of clusters.\n",
    "\n",
    "### Understanding Sample Data\n",
    "\n",
    "Recall from Week 7 that we can organize sample data in a data matrix (a table) where:\n",
    "* Each row represents a specific thing observed (e.g., an employee in the table below)\n",
    "* Each column represents a variable (e.g., employee ID, tenure, leadership, title in the table below)\n",
    "\n",
    "\n",
    "| Employee_ID | Tenure_Months | Leadership | Title | \n",
    "|:----|:---- |:---- |:---- |\n",
    "| e1 | 45 | $$True$$ | Project Manager |\n",
    "| e2 | 2 | $$False$$ | Junior Analyst |\n",
    "| e3 | 13 | $$False$$ | Adminstrative Assistant |\n",
    "| . | . | . | . |\n",
    "| . | . | . | . |\n",
    "| e63 | 19 | $$True$$ | Senior Engineer |\n",
    "\n",
    "\n",
    "#### Types of Variables\n",
    "\n",
    "The variable type represented in a column determines our options for analysis. Stated another way, the choice of appropriate statistical procedure, including visualization techniques, depends on the data type. \n",
    "\n",
    "Data can be categorical or numerical. If the variables are numerical, we are led to a certain statistical strategy. In contrast, if the variables represent qualitative categorizations, then we follow a different path.\n",
    "\n",
    "* Numerical - numeric values\n",
    "  * Discrete - integers, counting numbers, etc. (e.g., 1, 2, 3,)\n",
    "  * Continuous - real values (e.g., -1.2, $\\pi$, 5.2)\n",
    "* Categorical - non-numeric values\n",
    "  * Ordinal - values are classified in categories that can be ordered and have a logical sequence (e.g., tall, medium, short; very few, some, very many)\n",
    "  * Nominal - values are classified in categories that cannot be ordered (e.g., red, blue, green; married, single, divorced)\n",
    "    * Boolean - can only have two possible values (e.g., 0/1; yes/no; smoker/non-smoker; True/False)\n",
    "\n",
    "In the fictional employee data above, we might be interested in analyzing the relationship between tenure, leadership, and title. One of our first steps towards analysis is determining variable type. \n",
    "\n",
    "Question: *What are the variable types for tenure, leadership, and title as displayed in the data table above?*\n",
    "\n",
    "<details>\n",
    "    <summary><font color=\"blue\">Click here to see the answer</font></summary>\n",
    "\n",
    "* Tenure: Numerical, discrete\n",
    "* Leadership: Categorical, boolean\n",
    "* Title: Categorical, nominal\n",
    "</details>\n",
    "\n",
    "### Other Important Topics\n",
    "\n",
    "Comprehensive discussion of probabilities is well outside the scope of this course. However, if you would like to learn more or need a refresher, check out the freely available text by Grimstead and Snell, [Introduction to Probability](https://math.dartmouth.edu/~prob/prob/prob.pdf).\n",
    "\n",
    "## Descriptive Statistics\n",
    "\n",
    "Descriptive analysis typically deals with *data understanding* and *data preparation*. This involves identifying the data for a project, collecting the data, assessing the quality of the data, preparing the data (cleaning, transforming, converting), summarizing the data (often with visualizations), and finally selecting the final variables and observations. We can think of descriptive analysis as a process aimed at describing the data. \n",
    "\n",
    "Descriptive statistics utilizes summaries of data. These include counts, minimums, maximums, mean, median, mode, standard deviation, and others. These summaries do not infer anything about the data, but *describe* the characteristics of the data.\n",
    "\n",
    "|Measure of| Statistic  | Description | \n",
    "|:----|:----|:---- |\n",
    "|*Center*| Mean | Statistical average of numeric data |\n",
    "| | Median | Middle-most value of numeric data, when sorted  |\n",
    "|| Mode | Most common value of categorical data |\n",
    "|| Proportion | Frequency with which some categorical value occurs |\n",
    "|*Spread*| Range | Difference between max and min values of numerical data |\n",
    "|| Quartiles | Values occurring at the $\\frac{1}{4}$ ($Q_1$), $\\frac{1}{2}$ ($Q_2$), and $\\frac{3}{4}$ ($Q_3$) positions in numeric data, when sorted |\n",
    "|| Interquartile Range | Difference between the $Q_3$ and $Q_1$ quartile points in numeric data |\n",
    "|| Variance | Average squared distance between each point and the mean |\n",
    "|| Standard Deviation | Square root of the variance |\n",
    "\n",
    "As mentioned above, visualization is a key component of conducting any analysis. Visualizations are graphs, plots, figures, and other visual representations of the data that provide a summarized perspective on the data. The topic of data visualization deserves its own course as it is a complex and critical process in analytics, blending science and art to help us make sense of data and support claims based on data. For our course, we are limited to a cursory review of visualization.\n",
    "\n",
    "Let's discuss how to compute various descriptive statistics in Python. We set up our notebook at the beginning of this tutorial (importing modules and data, and setting our working directory), so we are ready to go.\n",
    "\n",
    "There are many ways to assess the basic descriptive information of data. Python's pandas library provides two functions that behave similarly to `summary()` and `str()` in R. These two functions are `describe()` and `dtypes`. The former provides an overview like `summary()`, providing basic descriptive information for numeric and categorical data. The attribute, `dtypes`, only provides the datatype of the columns, unlike `str()` in R that provides more information beyond datatypes.\n",
    "\n",
    "Run the code cells below to use `describe()` with the cars and ozone data sets. The output provides count, mean, standard deviation, minimum value ($Q_0$), the values for the 25th ($Q_1$), 50th ($Q_2$), and 75th ($Q_3$) quartiles, and the maximum value ($Q_4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c067b9-64e5-4343-9ae7-90c5ea46f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d7cca-5b25-476b-8c60-708b6f4a0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60502aa7-512b-46b2-adea-b3bea44730f0",
   "metadata": {},
   "source": [
    "Recall that the car data contains eight columns of data. But, the output for `car_df.describe()` only presents six columns worth of descriptive data. Why?\n",
    "\n",
    "The reason is, the two columns left out are string/categorical, not numeric. The `describe()` function in pandas has additional arguments (and some beyond these):\n",
    "\n",
    "* `describe(include=['object'])`: focuses just on string data\n",
    "* `describe(include=['category'])`: focuses just on categorical data\n",
    "* `describe(include=['number'])`: solely looks at numerical data\n",
    "* `describe(include='all')`: forces Python to assess both types\n",
    "\n",
    "\n",
    "Let's return to the car data and use the `include=['object']` argument. Note the differences of the output. We can confirm earlier findings by using the argument `include=['number']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0f013-7a6b-4c98-9cf8-15d77e942d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74a99a-8b02-497c-9a08-68d3a351a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.describe(include=['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01586ec4-f07e-4248-b9b1-6aec158e6abc",
   "metadata": {},
   "source": [
    "A quick aside, in previous tutorials, I included whitespace around the `=` operator in function parameters. I did this to emphasize the value assigned to a parameter. However, the PEP8 style guide recommends an absence of spaces around this operator in function parameter assignments in order to distinguish them from other types of assignment (e.g., variable assignment). \n",
    "\n",
    "Okay, back to our data. Recall that when pandas imports data into Python as a DataFrame, it does not automatically convert columns into categorical datatypes. To do so, we must convert the datatype of a column into categorical and then append it to the DataFrame. \n",
    "\n",
    "Let's convert *Country* column in the car data set into a categorical datatype and append it to the DataFrame. We will then use `columns` to view our newly created column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d2d5c-8852-494d-a609-ca68e0972142",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df['Country_categorical'] = car_df['Country'].astype('category')\n",
    "\n",
    "car_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ea7e0-6cc4-4218-857b-ba4fb4e75a5f",
   "metadata": {},
   "source": [
    "Looking at the listed columns, the newly created column `Country_categorical` is listed at the end. \n",
    "\n",
    "The datatype of `Country` is still `object`, while the datatype of `Country_categorical` is `category`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd8b9c-3610-40d5-95a4-00b987758e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63a912-fff4-49c3-8ab7-78f6208dabcc",
   "metadata": {},
   "source": [
    "Let's repeat the earlier process for `describe()`, only this time for data types of `object` and `category`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9857b-b5fd-4e18-8a58-1f3c03fc8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9b0de-5f6a-40ab-8741-dcde9332df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.describe(include=['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e34ffc-3c4b-4821-98e6-937e4e1345dc",
   "metadata": {},
   "source": [
    "The following table provides a list of the most fundamental descriptive functions within the pandas library, including those for skewness and kurtosis (both of these are measures of distribution *shape*).\n",
    "\n",
    "| Function | Description |\n",
    "|:---|:---|\n",
    "| count() | Number of non-null observations |\n",
    "| sum() | Sum of values |\n",
    "| mean() | Mean of values |\n",
    "| mad() | Mean absolute deviation |\n",
    "| median() | Arithmetic median of values |\n",
    "| min() | Minimum |\n",
    "| max() | Maximum |\n",
    "| mode() | Mode |\n",
    "| abs() | Absolute Value |\n",
    "| prod() | Product of values |\n",
    "| std() | Sample standard deviation |\n",
    "| var() | Unbiased variance |\n",
    "| sem() | Standard error of the mean |\n",
    "| skew() | Sample skewness (3rd moment) |\n",
    "| kurt() | Sample kurtosis (4th moment) |\n",
    "| quantile() | Sample quantile (value at %) |\n",
    "\n",
    "In order to determine the skewness and kurtosis within Python, separate functions must be run. Similar to R, these functions can be run separately for each column or on the entire DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a792e57e-84de-442a-a899-34177e62187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get kurtosis value for the Mileage variable in car data set\n",
    "car_df[\"Mileage\"].kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255e673-e59c-42f9-84c4-77caed574a85",
   "metadata": {},
   "source": [
    "Many other useful functions exist for categorical data within pandas. For example, it is possible to add additional categories, remove categories not currently used in the dataset, change the existing categories, and consolidate categories. [Check out the pandas documentation for more details on how to perform these operations](https://pandas.pydata.org/docs/user_guide/categorical.html).\n",
    "\n",
    "### Plots in Python\n",
    "\n",
    "Often, numbers on their own may not be intuitive. Humans are fine-tuned to interpret visual objects more readily than numerical data. It can be helpful to create plots to assess your data in addition to looking at summary statistics. The variable types in data inform our selection of data visualization types:\n",
    "\n",
    "* Numeric data\n",
    "  * Scatterplot: useful for visualizing the relationship between two numerical variables.\n",
    "  * Histogram: provide a view of the data density and help describe distribution shape; sensitive to bin width.\n",
    "  * Boxplot: the box in a box plot represents the middle 50% of the data, and the thick line in the box is the median.\n",
    "* Categoric data\n",
    "  * Bar plot: a common way to display a single categorical variable proportions or frequencies.\n",
    "    * Stacked bar plot: extends the standard bar chart from looking at numeric values across one categorical variable to two categorical variables.\n",
    "  * *Please never use a pie chart.*\n",
    "\n",
    "##### How are bar plots different than histograms?\n",
    "\n",
    "Bar plots are used for displaying distributions of categorical variables, while histograms are used for numerical variables. The x-axis in a histogram is a number line, hence the order of the bars cannot be changed, while in a bar plot the categories can be listed in any order (though some orderings make more sense than others, especially for ordinal variables).\n",
    "\n",
    "Below is an example of a simple plot for time-ordered data using the function `plot()`. The code below generates a random time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a94b6-96af-4e5e-a885-46d85040f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\n",
    "ts = ts.cumsum()\n",
    "ts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9f3b7-88e4-4181-9d17-4bce08dcdede",
   "metadata": {},
   "source": [
    "#### Scatterplot\n",
    "\n",
    "The scatter plot is a popular plot used to assess data prior to modeling. The scatter plot is simple enough to use within Python. For this example, variables from the ozone dataset will be used. The variable *radiation* is placed on the *y*-axis; the variable *temperature* on the *x*-axis. This allows us to assess the relationship between variables prior to modeling. \n",
    "\n",
    "In general, variables might be:\n",
    "* *Negatively* associated - as one one increases in value or proportion, the other tends to decrease.\n",
    "* *Positively* associated - as one one increases in value or proportion, the other also tends to increase.\n",
    "\n",
    "In the plot below, a weak positive relationship exists between *radiation* and *temperature*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c880a50-d912-43cd-b158-f2493bbf3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_df.plot.scatter(x='temp', y='rad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf52c7-0b51-4bfb-b8df-c35183cec689",
   "metadata": {},
   "source": [
    "#### Stats Sidebar: Correlations\n",
    "\n",
    "We can also use a *measure of relationship*, a correlation coefficient, to assess this association. A correlation coefficient measures the strength and direction of a (linear) relationship.\n",
    "\n",
    "In the code cell below, we use the `corr()` function provided in the pandas library. NumPy and SciPy also provide functions for computing correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98f839-b4bf-49c1-aa63-cdda129b7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_df['temp'].corr(ozone_df['rad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147f24c-36cc-447f-abae-f5b4e258da01",
   "metadata": {},
   "source": [
    "What if we want to inspect the correlations between all variables in the ozone data set? The syntax is simple with pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b659533-b1b2-474d-b38e-6839972dad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18235b83-315e-492f-8f94-19c57399cab9",
   "metadata": {},
   "source": [
    "#### Stats Sidebar: Explanatory and Response Variables\n",
    "\n",
    "When two variables are associated:\n",
    "* The first variable might affect the second.\n",
    "* The second variable might affect the first.\n",
    "* An unknown, confounding variable might affect both.\n",
    "\n",
    "The relationship between variables might occur because of chance or bias.\n",
    "* Explanatory variable - one that is suspected of affecting another.\n",
    "* Response variable - one that is suspected of being affected.\n",
    "  * explanatory variable &rarr; response variable\n",
    "* Labeling variables in this way guarantees nothing.\n",
    "* Correlation does not imply causation!\n",
    "\n",
    "<div><center>\n",
    "  <img src = \"https://imgs.xkcd.com/comics/correlation.png\">\n",
    "  <a href=\"https://xkcd.com/552/\">https://xkcd.com/552</a>\n",
    "</center></div>\n",
    "\n",
    "##### Look Hard Enough, There Will be Correlation\n",
    "\n",
    "* The real world involves many, many variables that interrelate in complex ways.\n",
    "* Also, chance is only \"random\" if you aren’t already looking for it.\n",
    "* If you want to find correlation and you look hard enough, then you will find it.\n",
    "* So beware of confirmation bias.\n",
    "* Or just weirdness.\n",
    "\n",
    "#### Multiple Plots\n",
    "\n",
    "If a data set contains more than one or two variables, it may take too much time to create individual scatter plots. In R, the function `pairs()` allows us to plot scatter plots for all variables. Unfortunately, pandas does not contain such a function. We will need to plot each pair of variables. Using our knowledge of loops, we can make short work of this. I will leave that up to you to explore how it is done.\n",
    "\n",
    "#### Boxplot\n",
    "\n",
    "Another type of plot that we can use is the boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af0631-1d43-4ef1-b153-db0d66983a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88347f3-e811-4807-a203-0cafd1df861a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "In this boxplot, we can see that outliers are present. Specifically, for both *wind* and *ozone*, data points lie above the horizontal bar. These are considered outliers in the data. These are data points that are either greater than 1.5 times the interquartile range or lower than 1.5 times the interquartile range. Though, *wind* is cramped making assessment difficult. Let's try removing *radiation* from the plot to bring that into focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137bb5f-f055-47d2-b4bc-0433f39db7cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "ozone_df.loc[:,['temp','wind','ozone']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133b133-d9bc-4df1-b1e8-0ba6c27a877d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "That is an improvement. *Ozone* is very much skewed with outliers. Still, *wind* is still squished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4accddca-8e5a-4e29-b43c-a8c4db7302b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "ozone_df.loc[:,['wind']].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e9991-e933-423d-b366-b97389e7f92a",
   "metadata": {},
   "source": [
    "Excellent! This brings it into greater focus. You can see the outliers present at the top of the plot.\n",
    "\n",
    "## Stats Sidebar: Outliers\n",
    "\n",
    "Very often, when analyzing distributions, what we are really looking for is what is most unusual (outliers). This is why some types of plots make seeing outliers explicit (e.g., boxplots and strip plots). When there are extreme outliers, we often need to be more careful about other measures - for instance, median is a measure of center that is more resistant to the effects of outliers compared to mean. In other words, extreme values have a strong effect on mean.\n",
    "\n",
    "#### Histograms \n",
    "\n",
    "Histograms are excellent for inspecting the *spread* of your data. A histogram provides the frequency in which certain data points appear. This also lets you eyeball the skewness and kurtosis of your data. We sometimes call this \"eyeball analysis\", where we use visual inspection to estimate something about data. This is a quick way to understand data but we should ultimately rely on precise and accurate calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf49a8a-a7d5-447a-8e7f-f915a66f4f08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "ozone_df['rad'].plot.hist(alpha=0.5)  # the alpha parameter is used to set the transparency level and ranges from 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f79aa-01a7-4399-b6fb-e00d04008474",
   "metadata": {},
   "source": [
    "#### Bar Chart\n",
    "\n",
    "We can use bar charts to plot categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba12c3-76a1-475c-9e35-5be5aa526bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical bar chart for car country\n",
    "car_df.Country.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a3d23-31e5-4825-a5ed-ea7e74b876c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal bar chart for car country\n",
    "car_df.Country.value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c5902-8a01-4fa6-b5db-6b5dea83ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal bar chart for car type\n",
    "car_df.Type.value_counts().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425c87c-e797-4624-973b-f97e6c159021",
   "metadata": {},
   "source": [
    "Note that the first two plots use the exact same data. One is a vertical bar chart while the other is horizontal. \n",
    "\n",
    "Vertical bar charts are especially well suited for plotting ordinal data where there is a logical, natural sequence (e.g., age groups, income brackets, etc.), allowing us to exploit the way we perceive increasing values or time for efficient, fast processing of information.\n",
    "\n",
    "Horizontal bar charts are well suited for plotting nominal variables, allowing us to easily read category labels and inspect numerical differences between those categories. \n",
    "\n",
    "#### Matplotlib\n",
    "\n",
    "You can check out a variety of example plots and their implementation in the [matplotlib gallery](https://matplotlib.org/stable/gallery/). Click on a plot to view the corresponding code.\n",
    "\n",
    "### From Descriptive to Inferential\n",
    "\n",
    "In analytics work, we first make use of descriptive statistics to understand our data. This includes (1) aggregating and sampling data to be analyzed with inferential statistics and (2) uncovering opportunities and constraints for selecting approaches and techniques for statistical modeling of our data. If the final set of data selected at the end of our descriptive, exploratory analysis contains errors or the wrong scope for the question asked, then the inferential statistics techniques applied to the data will incorrectly answer the question of interest. This is why a lot of the resources and time for data analysis are focused on data description and summarization. Get it wrong, and the rest of the project suffers. With that said, we now shift our discussion to inferential statistics.\n",
    "\n",
    "## Inferential Statistics\n",
    "\n",
    "We use inferential statistics in both predictive and prescriptive analysis. \n",
    "\n",
    "In *predictive* analysis, we apply statistical procedures and techniques to analyze some subset of data. That subset may have been selected when computing descriptive statistics and inspecting data (a posteriori - from observation), or it may be based on theory and hypotheses that guided data collection and preparation (a priori - from deduction). \n",
    "\n",
    "Predictive analysis involves developing models that predict or forecast outcomes. Examples of these outcomes include purchasing behavior, flight pathway of an airline, stock market trends, and much more. Models are assessed for viability, reliability, and consistency across contexts and domains.\n",
    "\n",
    "In *prescriptive* analysis, we also build models, but with a different goal in mind. These models are developed to help us evaluate and recommended changes to organizational decisions and processes. \n",
    "\n",
    "Predictive analysis often focuses on what *will* happen, rather than suggesting what *should* happen like in prescriptive analysis. Many of the statistical models utilized in prescriptive analysis are the same as those in predictive analysis. \n",
    "\n",
    "In this section of the tutorial, we will focusing on how to build simple regression models, including logistic regression.\n",
    "\n",
    "### Assumption Validation\n",
    "\n",
    "When performing regression, we need to meet a set of *assumptions*. These assumptions are based on the conditions that need to be met in order to determine if the selected model is appropriate for the data. If the data are not appropriate for the model, then the constructed model and associated results are not valid. \n",
    "\n",
    "For regression models, we must meet the following criteria: \n",
    "* Linearity: linear relationship exists\n",
    "* Correlation: the independent variables (i.e., explanatory or predictor variables) are not related to each other\n",
    "* Homoscedasticity: residuals exhibit a constant variance\n",
    "* Independence: residuals are not related to each other\n",
    "* Normality: residuals exhibit a normal distribution\n",
    "\n",
    "\n",
    "### Linearity\n",
    "\n",
    "To check the linearity of radiation against ozone concentration, we can create and inspect the scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7e368-f1b0-42c7-9b7d-e6e3463b4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_df.plot.scatter(x='rad', y='ozone')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947d2b3-48d8-4d91-8f96-94f95383c273",
   "metadata": {},
   "source": [
    "This is fairly linear and does not exhibit any curvature or other anomalies that would raise a red flag. Linearity is not an issue.\n",
    "\n",
    "\n",
    "### Correlation\n",
    "\n",
    "The next assumption to check is that of multicollinearity. The first method is correlation analysis. The code cells below provide two examples. The first code cell provides correlation comparisons for all variables without *p*-values (we computed this earlier). The subsequent code cells provides *p*-values, but for a single pair of variables per cell. You could place each of these in a single code cell but you will need to wrap each line with the `print()` function or change your default notebook behavior to display all output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7520cde-d920-440d-952b-09b3f17ed85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozone_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11ca8e-c4dc-451a-b461-8da6549c120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ozone_df.rad, ozone_df.ozone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c69d10-ca8a-4fd1-8efa-ecd40aeb22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ozone_df.wind, ozone_df.ozone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1f2ae-0b77-4a62-b668-6734d07e323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ozone_df.temp, ozone_df.ozone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97320fd-27b1-4b88-8673-c5426b94e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ozone_df.rad, ozone_df.wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f4a0b-66c7-4b40-9953-c1b2c0369c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ozone_df.rad, ozone_df.temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99734a-518b-4331-9d74-7e5b0a8bb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(ozone_df.wind, ozone_df.temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7df92-9be8-4414-af37-4a9e22da8d69",
   "metadata": {},
   "source": [
    "An alternative method to assessing multicollinearity is using a variance inflation factor, or VIF for short. Like in R, the regression is performed prior to calculating the VIF scores. This assumes we have already created a regression object `lin_reg_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34876445-e49e-45ce-a89f-3e5deeeac5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the sklearn library for regression\n",
    "lin_reg_1 = LinearRegression(fit_intercept=True)\n",
    "lin_reg_1.fit(ozone_df[['rad', 'wind', 'temp']], ozone_df.ozone)\n",
    "\n",
    "# calculate VIF for radiation\n",
    "lin_reg_1.fit(ozone_df[['wind', 'temp']], ozone_df.rad)\n",
    "vif1 = 1/(1 - lin_reg_1.score(ozone_df[['wind', 'temp']], ozone_df.rad))\n",
    "\n",
    "# calculate VIF for wind\n",
    "lin_reg_1.fit(ozone_df[['rad', 'temp']], ozone_df.wind)\n",
    "vif2 = 1/(1 - lin_reg_1.score(ozone_df[['rad', 'temp']], ozone_df.wind))\n",
    "\n",
    "# calculate VIF for temperature\n",
    "lin_reg_1.fit(ozone_df[['rad', 'wind']], ozone_df.temp)\n",
    "vif3 = 1/(1 - lin_reg_1.score(ozone_df[['rad', 'wind']], ozone_df.temp))\n",
    "\n",
    "# output VIF scores\n",
    "print('VIF rad: ', vif1,\n",
    "        '\\nVIF wind: ', vif2,\n",
    "        '\\nVIF temp: ', vif3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65f32d-36a4-444b-8fd1-3a619bbaf1e3",
   "metadata": {},
   "source": [
    "The regression library does not contain a function to calculate the VIF scores. The code above is a simple calculation for each variable.\n",
    "\n",
    "### Constant Variance: Homoscedasticity\n",
    "\n",
    "The third assumption assessed for regression is homoscedasticity, or constant variance. Unfortunately, the regression function `LinearRegression().fit()` from the library `sklearn` cannot provide the predicted values in order to generate the plot. Instead, we must use the library `statsmodels` and its function `ols().fit()`. OLS is shorthand for Ordinary Least Squares. This approach attempts to fit a model that minimizes the square of distance of each point from the model regression line.\n",
    "\n",
    "Please note, the `sklearn` and `statsmodels` libraries require overlapping support libraries which can cause errors if you use both at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea870051-3409-4e58-9950-3fc2f70f2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_2 = smf.ols('ozone ~ rad + wind + temp', ozone_df).fit()\n",
    "\n",
    "# assess homoscedasticity\n",
    "plt.scatter(lin_reg_2.fittedvalues, lin_reg_2.resid)\n",
    "plt.xlabel('Predicted/Fitted Values')\n",
    "plt.ylabel('Residual Values')\n",
    "plt.title('Assessing Homoscedasticity')\n",
    "plt.plot([-40, 120],[0, 0], 'red', lw=2)   # add red horizontal line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b715257-adfd-4c92-ba84-46bdb810d0f1",
   "metadata": {},
   "source": [
    "In the plot above, the red line is generated by a line of code. The data points are not evenly distributed across the x-axis. Several data points appear to be causing this large spread. These data points are most likely outliers.\n",
    "\n",
    "### Independence\n",
    "\n",
    "The fourth assumption to check is that of independence of residuals. Within Python, the Durbin-Watson test statistic is obtained by viewing the summary results of the regression object. This output comes standard along with the other output. The Durbin-Watson test statistic values in the range of 1.5 to 2.5 are relatively acceptable. Values outside of this range could be a cause for worry. Values under 1 or more than 3 are a definite cause for worry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe7838-0bc1-48a5-8435-f0fcf77145a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74106b01-a260-4c94-9006-a1c0707e0da2",
   "metadata": {},
   "source": [
    "### Normally Distributed Residuals\n",
    "\n",
    "The last assumption is that of normally distributed residuals. This can be assessed by creating a QQ Plot. The following code cell generates a \"Probability Plot\" or QQ plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f978e-7fe6-4999-9ae2-08983a8e46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.probplot(lin_reg_2.resid, dist=\"norm\", plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720ff0f-c9c6-4c11-8c39-30886b095802",
   "metadata": {},
   "source": [
    "The straight, dashed line represents a normal distribution; the circles represent the data points of your variable. Notice the data points at the tail end. These are the same data points that stood out in the test for constant variance.  \n",
    "\n",
    "In addition to the QQ plot, you can use the Shapiro-Wilk test to evaluate normality: \n",
    "\n",
    "```Python\n",
    "sts.shapiro(ozone_df.rad)  # check normality of radiation variable in the ozone data set\n",
    "```\n",
    "\n",
    "This is considered a more objective assessment and provides a *p*-value. Normality results in a non-significant result of the test. Importantly, the significance depends on the alpha level you choose. A more conservative approach is recommended: require an alpha of 0.05. This function also comes from the library `scipy.stats`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814c005-20d6-431c-8a3e-a27271c591e7",
   "metadata": {},
   "source": [
    "### Regression in Python\n",
    "\n",
    "We actually already built regression models above! In any case, we have two libraries to choose a regression function. Some folks may prefer the elegant output provided by `statsmodels` over `sklearn`. To be frank, I do not have a preference between the two libraries because I prefer R for the majority of my statistical analysis.\n",
    "\n",
    "In the code below, ozone is our response variable and radiation, wind, and temperature are the selected explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac048f3b-15ea-4ae4-9d84-68090acc324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_2 = smf.ols('ozone ~ rad + wind + temp', ozone_df).fit()\n",
    "\n",
    "lin_reg_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9df0fb-1a61-45f4-9252-ed48a9a802dc",
   "metadata": {},
   "source": [
    "The *F*-statistic with its accompanying *p*-value is found on the right-hand side of the output. The *t*-tests are located toward the bottom along with the *p*-values and estimates, or coefficients. The output also contains the R-Squared ($R^2$) and the (more important) Adjusted $R^2$. Lastly, a bonus is the AIC and BIC fit indices. These fit indices are meaningful when compared to the fit indices of other models (i.e., when you are trying to select the best model from a set).\n",
    "\n",
    "* AIC - Akaike Information Criterion\n",
    "* BIC - Bayesian Information Criterion\n",
    "\n",
    "#### Problems with R-Squared\n",
    "\n",
    "$R^2$ tells us how much variance in our dependent or response variable can be explained by the model in question. However, it has limitations.\n",
    "\n",
    "* Every time you add a predictor to your model, $R^2$ will tend to go up, even if by chance.\n",
    "* With too many predictors and too large polynomials, the model simply memorizes the data (chance and all) - *overfitting*.\n",
    "* Adjusted $R^2$ compensates (somewhat) for the number of predictors used in the model.\n",
    "* Predicted $R^2$ tells us how strongly correlated the model is over new data (not directly modeled) - it is a measure of *generalization* of the model.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "For this section of the tutorial, we will use the the student performance data set. In Python, the first task to be done is convert the As and Bs to numerical values. The logit function within `statsmodels` only accepts 0s and 1s as input for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ca598-ce2b-483a-b107-f1ea253135bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine what unique values Grade has\n",
    "student_df.Grade.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ac6a6-bc9c-41e8-a392-fcfa984e9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change values\n",
    "student_df.Grade = (student_df.Grade.replace(['A','B'], [0,1])\n",
    "                    .infer_objects(copy=False))  # note that a warning message will be printed about a deprecated method, you can ignore this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d4b37-f722-4d3e-aa0e-9ec1c3eff33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm change\n",
    "student_df.Grade.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df62a4-5f94-431c-a5a2-e570f63bf386",
   "metadata": {},
   "source": [
    "The original unique values were listed as “A” and “B” prior to the conversion; once changed, the unique values are switched over to `0` and `1`. A strong caution needs to be added here. If the datatype of the variable is categorical, you cannot simply change the values to something different. This is because the metadata of the categorical variable most likely does not contain the new values you are using. Convert the variable to an object datatype, replace the values, then convert it back to a categorical.\n",
    "\n",
    "Okay, let's perform logistic regression. In the code below, Grade is our response variable and Project and Exam are the selected explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacab990-8286-43a4-9eec-f8cd1a7527d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_2 = smf.logit('Grade ~ Project + Exam', student_df).fit()\n",
    "\n",
    "log_reg_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2df7fc-61fa-4d13-8cd6-a4804af214ca",
   "metadata": {},
   "source": [
    "The results indicate that both `Project` and `Exam` are influential in students obtaining a grade within the classroom.\n",
    "\n",
    "---\n",
    "\n",
    "Note that there are a variety of ways to perform regression (different models and different data types). For example, we did not cover regression with categorical data. As this is not a statistics course, we have only covered some basics on the implementation of regression in Python. As mentioned earlier, I prefer R for this type of statistical modeling. It is ultimately up to you to determine which tools and languages work best for you and/or are required for your organizational role.\n",
    "\n",
    "---\n",
    "\n",
    "That wraps up this week's tutorial. Some of the information presented here is based on the freely available textbook [OpenIntro Statistics](http://www.openintro.org/stat/) in addition to materials sourced from UM's Dr. Hammer and one of my favorite instructors from my days as a graduate student, [Dr. R. Paul Wiegand](https://www.winthrop.edu/cbt/faculty/Wiegand-Paul.aspx).\n",
    "\n",
    "Next week we will cover some advanced statistical techniques. See you then!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
